{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!---- deploy info---->\n",
    "\n",
    "# 線形回帰の基礎\n",
    "\n",
    "------------\n",
    "## このテキストについて\n",
    "\n",
    "### テキストの目的\n",
    "\n",
    "- 線形回帰の基礎を知る\n",
    "- 行列計算の意義を知る\n",
    "- 理論から実装への橋渡し\n",
    "\n",
    "### どのように学ぶか\n",
    "\n",
    "まずは一変数で線形回帰の基本を考えてます。そこから線形代数を利用して多変数に発展させていきます。\n",
    "\n",
    "------------\n",
    "\n",
    "## 線形回帰とは\n",
    "\n",
    "複数の特徴量を持ったデータの回帰をいきなり考えるのはとても難しいので、まずは特徴量を一つとして考えてみます。こういうものを特に「単回帰」といいます。\n",
    "\n",
    "仮定関数を $ \\hat{y} = ax + b $ として、予測値 $\\hat{y}$ と実測値(教師データ) $y$ の二乗距離の和を最小にするアルゴリズムが最小二乗法です。\n",
    "\n",
    "こういったアルゴリズムによってパラメータを決定し、引くべき直線を予測する営み全体のことを線形回帰といいます。\n",
    "\n",
    "# ここ画像欲しい\n",
    "\n",
    "### 最小二乗法\n",
    "\n",
    "まず簡単にサンプル $i \\in (1,2,3,...,n)$ における二乗距離(L2ノルム)の二乗を求めてみます。\n",
    "\n",
    "\\begin{align}\n",
    "  J_i &= (y_i - \\hat{y}_i)^2  \\\\\n",
    "      &= (y_i - (ax_i + b))^2  \\\\\n",
    "      &= y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2  \\\\\n",
    "\\end{align}\n",
    "\n",
    "$n$ : サンプルサイズ\n",
    "\n",
    "一つのサンプルだけでなく、全てのサンプルでの評価値を足します。\n",
    "\n",
    "\\begin{align}\n",
    "    J &= \\sum_{i=1}^{n} J_i  \\\\\n",
    "      &= \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)  \\\\\n",
    "      & \\propto \\frac{1}{n} \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)  \\\\\n",
    "      & \\propto \\frac{1}{2n} \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)\n",
    "\\end{align}\n",
    "\n",
    "これが単回帰における目的関数(損失関数)です。\n",
    "\n",
    "最後に比例関係を表す式を書いていますがそれぞれの理由を説明します。\n",
    "\n",
    "\\begin{align}\n",
    "    J = \\frac{1}{n} \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)\n",
    "\\end{align}\n",
    "\n",
    "こちらが評価関数といわれるもので、単純にサンプル数で割っています。評価はサンプル数に依らないはず。という信念によるものです。同じ環境下で、サンプルが増えれば増えるほど、評価が悪くなるということはないはずだからです。\n",
    "\n",
    "\\begin{align}\n",
    "    L = \\frac{1}{2n} \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)\n",
    "\\end{align}\n",
    "\n",
    "こちらは $2n$ で割っています。この式は評価関数ではなく、目的関数(損失関数)と呼ばれることが多いです。今回はこれらを区別するためこちらを $L$ とします。この二つの式には質的に差はありません。ただの定数倍だからです。２でさらに割る理由としては単純に微分した形が綺麗な形になるためなので、更新式にはこちらを利用することが多いです。\n",
    "\n",
    "\n",
    "最急降下法は以下のように最適化したいパラメータについて微分し勾配を求め、学習率をかけて古いパラメータを更新していくものです。\n",
    "\n",
    "\\begin{align}\n",
    "  \\theta_{i}^{k+1} &= \\theta_{i}^{k} - \\alpha \\nabla L \\\\\n",
    "                   &= \\theta_{i}^{k} - \\alpha \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}\n",
    "\n",
    "それぞれのパラメータによる損失関数の微分は以下のようになります。一般に以下の式で定式化されます。\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial L}{\\partial a} &= \\frac{1}{n} \\bigr( -\\sum_{i=1}^{n} y_i x_i + \\sum_{i=1}^{n} (a x_i + b)x_i \\bigr)  \\\\\n",
    "  \\frac{\\partial L}{\\partial b} &= \\frac{1}{n} \\bigr( -\\sum_{i=1}^{n} y_i     + \\sum_{i=1}^{n} (a x_i + b) \\bigr)\n",
    "\\end{align}\n",
    "\n",
    "これを最急降下法に適用すると更新式ができます。\n",
    "\n",
    "もちろんこのまま計算してもいいですが、切片と係数では微妙に形が違いますね。これを解決するために、切片も係数と考え、この係数に対しては常に値1が与えられると考えることで統一的に処理できます。\n",
    "\n",
    "最終的に得られる更新式は\n",
    "\\begin{align}\n",
    "  a_{new} &= a_{old} - \\alpha \\Bigr( - \\sum_{i=1}^{n} y_i x_i + \\sum_{i=1}^{n} (a_{old} x_i + b_{old})x_i \\Bigl)  \\\\\n",
    "  b_{new} &= b_{old} - \\alpha \\Bigr( - \\sum_{i=1}^{n} y_i \\times 1 + \\sum_{i=1}^{n} (a_{ald} x_i + b_{old}) \\times 1 \\Bigl)\n",
    "\\end{align}\n",
    "\n",
    "上式を繰り返すことによって、$a$ と $b$ が適切な値になって行きます。この時繰り返し回数(イテレーション回数)と学習率によってはうまく収束しません。適切な大きさに設定する必要があります。\n",
    "\n",
    "### 線形回帰アルゴリズムへのつながり\n",
    "\n",
    "ではこれを実装につなげて行きましょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
