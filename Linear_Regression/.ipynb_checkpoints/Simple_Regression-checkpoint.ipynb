{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形回帰の基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習や統計学の基礎となる線形回帰のフルスクラッチをしていきましょう。  \n",
    "仮定関数を $ \\hat{y} = XW $ として、$i$番目の実測値である$y_i$との二乗距離(L2ノルム)$||y-\\hat{y}_i||_2$を最小になるようにするアルゴリズムが線形回帰です。\n",
    "仮定関数：$ \\hat{y} = XW $ の中身は以下のようになっています。サンプルを行方向に格納している形です。(格納の仕方によって行列の転置などを行う必要があるので注意してください)\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y} = \n",
    "        \\begin{bmatrix}\n",
    "            \\hat{y_1} \\\\\n",
    "            \\hat{y_2} \\\\\n",
    "            \\vdots \\\\\n",
    "            \\hat{y_n}\n",
    "        \\end{bmatrix},\n",
    "    W &= \n",
    "        \\begin{bmatrix}\n",
    "            w_0 \\\\\n",
    "            w_1 \\\\\n",
    "            w_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            w_d\n",
    "        \\end{bmatrix},\n",
    "    X = \n",
    "        \\begin{bmatrix}\n",
    "            x_{01} && x_{11} && \\cdots && x_{d1} \\\\\n",
    "            x_{02} && x_{12} && \\cdots && x_{d2} \\\\\n",
    "            \\vdots && \\vdots && \\ddots && \\vdots \\\\\n",
    "            x_{0n} && x_{1n} && \\cdots && x_{dn}\n",
    "        \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "\n",
    "つまり、i番目のサンプルに対する予測値は\n",
    "\\begin{align}\n",
    "\\hat{y}_i = w_0 x_{0i} + w_1 x_{1i} + w_2 x_{2i} + \\cdots + w_d x_{di}\n",
    "\\end{align}\n",
    "\n",
    "となっています。多くの場合は$w_0$は切片として設定し、$x_0i$はすべて1とすることが多いです。\n",
    "\n",
    "\n",
    "実測値(教師データ)と予測値の距離などを表すものを、「評価関数」、または「損失関数」といいます。  \n",
    "今回は二乗距離が最小になるようにしたいので、評価関数Jは、\n",
    "\n",
    "\\begin{align}\n",
    "  J &= \\frac{1}{2}||y-\\hat{y}||^2  \\\\\n",
    "    &= (y-\\hat{y})^T(y-\\hat{y})  \\\\\n",
    "    &= (y-XW)^T(y-XW)  \\\\\n",
    "    &= y^Ty - 2XWy^T + W^T X^T X W\n",
    "\\end{align}\n",
    "\n",
    "となります。実はこれを解析的に解いて係数行列$W$を導出することもできますが、最急降下法を用いて少しずつ係数を更新し、最適化します。\n",
    "最適化の更新式は以下のよおりです。\n",
    "\\begin{align}\n",
    "    \\frac{\\partial J}{\\partial w} &= -X^T(y-\\hat{y})\n",
    "    \\\\\n",
    "    w_{new} &= w_{old} - \\alpha \\frac{\\partial J}{\\partial w}  \\\\\n",
    "            &= w_{old} + \\alpha X^T(y-\\hat{y})\n",
    "\\end{align}\n",
    "この導出の行間は線形代数や最急降下法の勉強になるので、一度触れておくといいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
