機械学習や統計学の基礎となる線形回帰のフルスクラッチをしていきましょう。  
複数の特徴量を持ったデータの回帰をいきなり考えるのはとても難しいので、まずは特徴量を一つとして考えてみます。こういうものを特に「単回帰」といいます。
仮定関数を$\hat{y} = ax + b$として、予測した値$\hat{y}$と実測値(教師データ)の二乗距離の和を最小にするアルゴリズムが最小二乗法です。  
まず簡単にサンプル$i$における二乗距離(L2ノルム)の二乗を求めてみると、

$$
J_i = (y_i - \hat{y}_i)^2
$$
$$    = (y_i - (ax_i + b))^2 
    = y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2  \\
  J = \sum_{i=1}^{n} J_i  \\
    = \sum_{i=1}^{n} \bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\bigl)  \\
    \propto \frac{1}{2n} \sum_{i=1}^{n} \bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\bigl)
$$

これが目的関数(損失関数)です。  
サンプル数で割るのは、これを微分したものでパラメータを最適化するので、その更新幅がサンプル数に依存しないようにするためです。評価はサンプル数に依らないはず。という気持ちもあります。2で割っているのは微分したときに綺麗な形になるからです。  

最急降下法は以下のように最適化したいパラメータについて微分し勾配を求め、学習率をかけて古いパラメータを更新していくものです。  

\begin{aligned}
  \theta_{i}^{k+1} &= \theta_{i}^{k} - \alpha \nabla J \\
                   &= \theta_{i}^{k} - \alpha \frac{\partial J}{\partial \theta_i}
\end{aligned}

それぞれのパラメータでの微分は以下のようになります。

\begin{align}
  \frac{\partial J}{\partial a} &= -\sum_{i=1}^{n} y_i x_i + \sum_{i=1}^{n} (a x_i + b)x_i  \\
  \frac{\partial J}{\partial b} &= -\sum_{i=1}^{n} y_i     + \sum_{i=1}^{n} (a x_i + b)
\end{align}

これを最急降下法に適用すると更新式ができます。

もちろんこのまま計算してもいいですが、切片と係数では微妙に形が違いますね。これを解決するために、切片も係数と考え、この係数に対しては常に値1が与えられると考えることで統一的に処理できます。

最終的に得られる更新式は
\begin{align}
  a_{new} &= a_{old} - \alpha \Bigr( - \sum_{i=1}^{n} y_i x_i + \sum_{i=1}^{n} (a x_i + b)x_i \Bigl)  \\
  b_{new} &= b_{old} - \alpha \Bigr( - \sum_{i=1}^{n} y_i \times 1 + \sum_{i=1}^{n} (a x_i + b) \times 1 \Bigl)
\end{align}$$

まずはこの資料を参考に線形回帰の基礎を理解していきましょう。