{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形回帰の基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習や統計学の基礎となる線形回帰のフルスクラッチをしていきましょう。  \n",
    "複数の特徴量を持ったデータの回帰をいきなり考えるのはとても難しいので、まずは特徴量を一つとして考えてみます。こういうものを特に「単回帰」といいます。\n",
    "仮定関数(目的関数)を $ \\hat{y} = ax + b $ として、予測した値$\\hat{y}$と実測値(教師データ)の二乗距離の和を最小にするアルゴリズムが最小二乗法です。  \n",
    "まず簡単にサンプル$i$における二乗距離(L2ノルム)の二乗を求めてみると、\n",
    "\n",
    "\\begin{align}\n",
    "  J_i &= (y_i - \\hat{y}_i)^2  \\\\\n",
    "      &= (y_i - (ax_i + b))^2  \\\\\n",
    "      &= y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2  \\\\\n",
    "    J &= \\sum_{i=1}^{n} J_i  \\\\\n",
    "      &= \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)\n",
    "\\end{align}\n",
    "\n",
    "これが評価関数(損失関数)です。最適化したいパラメータは$a$と$b$なので、これについて微分し、最急降下法を用います。  \n",
    "最急降下法は以下のように最適化したいパラメータについて微分し勾配を求め、学習率をかけて古いパラメータを更新していくものです。  \n",
    "\n",
    "\\begin{align}\n",
    "  \\theta_{i}^{k+1} &= \\theta_{i}^{k} - \\alpha \\nabla J \\\\\n",
    "                   &= \\theta_{i}^{k} - \\alpha \\frac{\\partial J}{\\partial \\theta_i}\n",
    "\\end{align}\n",
    "\n",
    "これを単回帰の式に適用します。\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial J}{\\partial a} &= -2 \\sum_{i=1}^{n} y_i x_i + 2 \\sum_{i=1}^{n} (a x_i + b)x_i  \\\\\n",
    "  \\frac{\\partial J}{\\partial b} &= -2 \\sum_{i=1}^{n} y_i     + 2 \\sum_{i=1}^{n} (a x_i + b)\n",
    "\\end{align}\n",
    "\n",
    "もちろんこのまま計算してもいいですが、行列計算に落とし込んだほうがnumpyの行列計算を使えるので、\n",
    "\n",
    "\n",
    "\n",
    "計算が速くなります。\n",
    "\n",
    "実測値(教師データ)と予測値の距離などを表すものを、「評価関数」、または「損失関数」といいます。  \n",
    "今回は二乗距離が最小になるようにしたいので、評価関数Jは、\n",
    "\n",
    "\\begin{align}\n",
    "  J &= \\frac{1}{2}||y-\\hat{y}||^2  \\\\\n",
    "    &= (y-\\hat{y})^T(y-\\hat{y})  \\\\\n",
    "    &= (y-XW)^T(y-XW)  \\\\\n",
    "    &= y^Ty - 2XWy^T + W^T X^T X W\n",
    "\\end{align}\n",
    "\n",
    "となります。実はこれを解析的に解いて係数行列$W$を導出することもできますが、最急降下法を用いて少しずつ係数を更新し、最適化します。\n",
    "最適化の更新式は以下のとおりです。サンプル数に更新幅を依存させないため、サンプル数で割ることが多いです。\n",
    "\\begin{align}\n",
    "    \\frac{\\partial J}{\\partial w} &= -X^T(y-\\hat{y})  \\\\\n",
    "                                  &\\propto \\frac{-X^T(y-\\hat{y})}{n}\n",
    "    \\\\\n",
    "    w_{new} &= w_{old} - \\alpha \\frac{\\partial J}{\\partial w}  \\\\\n",
    "            &= w_{old} + \\alpha X^T(y-\\hat{y})\n",
    "\\end{align}\n",
    "この導出の行間は線形代数や最急降下法の勉強になるので、一度触れておくといいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#線形回帰のフルスクラッチ\n",
    "class ScratchLinearRegression():\n",
    "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3 (tags/v3.8.3:6f8c832, May 13 2020, 22:37:02) [MSC v.1924 64 bit (AMD64)]\n",
      "['C:\\\\Users\\\\GotoRei\\\\poetry-afro\\\\Scripts\\\\Youtube_AI\\\\Youtube_AI\\\\Linear_Regression', 'c:\\\\users\\\\gotorei\\\\poetry-afro\\\\.venv\\\\scripts\\\\python38.zip', 'c:\\\\python38\\\\DLLs', 'c:\\\\python38\\\\lib', 'c:\\\\python38', 'c:\\\\users\\\\gotorei\\\\poetry-afro\\\\.venv', '', 'c:\\\\users\\\\gotorei\\\\poetry-afro\\\\.venv\\\\lib\\\\site-packages', 'c:\\\\users\\\\gotorei\\\\poetry-afro\\\\.venv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\users\\\\gotorei\\\\poetry-afro\\\\.venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\users\\\\gotorei\\\\poetry-afro\\\\.venv\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\users\\\\gotorei\\\\poetry-afro\\\\.venv\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\GotoRei\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重回帰に挑戦する場合以下を参考にしてみてください。\n",
    "\n",
    "\n",
    "仮定関数を $ \\hat{y} = XW $ として、$i$番目の実測値である$y_i$との二乗距離(L2ノルム)$||y-\\hat{y}_i||_2$を最小になるようにするアルゴリズムが線形回帰です。\n",
    "仮定関数：$ \\hat{y} = XW $ の中身は以下のようになっています。サンプルを行方向に格納している形です。(格納の仕方によって行列の転置などを行う必要があるので注意してください)\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y} = \n",
    "        \\begin{bmatrix}\n",
    "            \\hat{y_1} \\\\\n",
    "            \\hat{y_2} \\\\\n",
    "            \\vdots \\\\\n",
    "            \\hat{y_n}\n",
    "        \\end{bmatrix},\n",
    "    W &= \n",
    "        \\begin{bmatrix}\n",
    "            w_0 \\\\\n",
    "            w_1 \\\\\n",
    "            w_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            w_d\n",
    "        \\end{bmatrix},\n",
    "    X = \n",
    "        \\begin{bmatrix}\n",
    "            x_{01} && x_{11} && \\cdots && x_{d1} \\\\\n",
    "            x_{02} && x_{12} && \\cdots && x_{d2} \\\\\n",
    "            \\vdots && \\vdots && \\ddots && \\vdots \\\\\n",
    "            x_{0n} && x_{1n} && \\cdots && x_{dn}\n",
    "        \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "\n",
    "つまり、i番目のサンプルに対する予測値は\n",
    "\\begin{align}\n",
    "\\hat{y}_i = w_0 x_{0i} + w_1 x_{1i} + w_2 x_{2i} + \\cdots + w_d x_{di}\n",
    "\\end{align}\n",
    "\n",
    "となっています。多くの場合は$w_0$は切片として設定し、$x_{0i}$はすべて1とすることが多いです。\n",
    "\n",
    "\n",
    "実測値(教師データ)と予測値の距離などを表すものを、「評価関数」、または「損失関数」といいます。  \n",
    "二乗距離が最小になるようにしたいので、評価関数Jは、\n",
    "\n",
    "\\begin{align}\n",
    "  J &= \\frac{1}{2}||y-\\hat{y}||^2  \\\\\n",
    "    &= (y-\\hat{y})^T(y-\\hat{y})  \\\\\n",
    "    &= (y-XW)^T(y-XW)  \\\\\n",
    "    &= y^Ty - 2XWy^T + W^T X^T X W\n",
    "\\end{align}\n",
    "\n",
    "となります。実はこれを解析的に解いて係数行列$W$を導出することもできますが、最急降下法を用いて少しずつ係数を更新し、最適化します。\n",
    "最適化の更新式は以下のとおりです。サンプル数に更新幅を依存させないため、サンプル数で割ることが多いです。\n",
    "\\begin{align}\n",
    "    \\frac{\\partial J}{\\partial w} &= -X^T(y-\\hat{y})  \\\\\n",
    "                                  &\\propto \\frac{-X^T(y-\\hat{y})}{n}\n",
    "    \\\\\n",
    "    w_{new} &= w_{old} - \\alpha \\frac{\\partial J}{\\partial w}  \\\\\n",
    "            &= w_{old} + \\alpha X^T(y-\\hat{y})\n",
    "\\end{align}\n",
    "この導出の行間は線形代数や最急降下法の勉強になるので、一度触れておくのも良いですね。\n",
    "では実装していきましょう"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
