{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!---- deploy info---->\n",
    "\n",
    "# ロジスティック回帰の基礎\n",
    "\n",
    "------------\n",
    "## このテキストについて\n",
    "\n",
    "### テキストの目的\n",
    "\n",
    "- ロジスティック回帰の基礎を知る\n",
    "- 行列計算の意義を知る\n",
    "- 理論から実装への橋渡し\n",
    "\n",
    "### どのように学ぶか\n",
    "\n",
    "まずは一変数で線形回帰の基本を考えます。そこから線形代数を利用して多変数に発展させていきます。\n",
    "\n",
    "------------\n",
    "\n",
    "## 線形回帰とは\n",
    "\n",
    "線形回帰とは、複数の特徴量に係数をかけて、それらを足し合わせた値が目的の値になるはず。という信念で考えられたもっとも単純なモデルです。\n",
    "\n",
    "複数の特徴量を持ったデータの線形回帰をいきなり考えると難しいので、まずは特徴量を一つとして考えてみます。こういうものを特に```単回帰```といいます。\n",
    "\n",
    "線形回帰ではパラメータ(特徴量にかける係数)を適切な値に設定しなければうまく予想できません。\n",
    "\n",
    "このパラメータを更新するための手法が、```最小二乗法```と```最急降下法```です。\n",
    "\n",
    "\n",
    "\n",
    "# ここ直線の画像画像欲しい\n",
    "\n",
    "## 単変量解析における線形回帰\n",
    "\n",
    "### 最小二乗法\n",
    "\n",
    "まず簡単にサンプル $i \\in (1,2,3,...,n)$ における二乗距離(L2ノルム)の二乗を求めてみます。\n",
    "\n",
    "考える仮定関数は $\\hat{y} = ax + b$ です。\n",
    "\n",
    "\\begin{align}\n",
    "  J_i &= (y_i - \\hat{y}_i)^2  \\\\\n",
    "      &= (y_i - (ax_i + b))^2  \\\\\n",
    "      &= y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2  \\\\\n",
    "\\end{align}\n",
    "\n",
    "一つのサンプルだけでなく、全てのサンプルでの評価値を足します。\n",
    "\n",
    "今回はサンプルサイズを $n$ とします。\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    J &= \\sum_{i=1}^{n} J_i  \\\\\n",
    "      &= \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)  \\\\\n",
    "      & \\propto \\frac{1}{n} \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)  \\\\\n",
    "      & \\propto \\frac{1}{2n} \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)\n",
    "\\end{align}\n",
    "\n",
    "これが単回帰における評価関数と損失関数です。\n",
    "\n",
    "最後に比例関係を表す式で二種類書いていますがそれぞれの理由を説明します。\n",
    "\n",
    "\\begin{align}\n",
    "    J = \\frac{1}{n} \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)\n",
    "\\end{align}\n",
    "\n",
    "こちらが評価関数といわれるもので、単純にサンプル数で割っています。評価はサンプル数に依らないはず。という信念によるものです。同じ環境下で、サンプルが増えれば増えるほど、評価が悪くなるということはないはずだからです。\n",
    "\n",
    "\\begin{align}\n",
    "    L = \\frac{1}{2n} \\sum_{i=1}^{n} \\bigr({y_i^2 - 2y_i(ax_i + b) + (ax_i + b)^2}\\bigl)\n",
    "\\end{align}\n",
    "\n",
    "こちらは $2n$ で割っています。この式は評価関数ではなく、損失関数と呼ばれることが多いです。今回はこれら二式を区別するためこちらを $L$ とします。この二つの式には質的な差はありません。ただの定数倍だからです。ですが、評価関数を２でさらに割ることで微分した形が綺麗な形になるため、更新式にはこちらを利用することが多いです。\n",
    "\n",
    "### 最急降下法\n",
    "\n",
    "最急降下法は以下のように最適化したいパラメータについて微分し勾配を求め、学習率をかけて古いパラメータを更新していくものです。\n",
    "\n",
    "\\begin{align}\n",
    "  \\theta_{i}^{k+1} &= \\theta_{i}^{k} - \\alpha \\nabla L \\\\\n",
    "                   &= \\theta_{i}^{k} - \\alpha \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}\n",
    "\n",
    "# ここも最急降下法のイメージ画像欲しい\n",
    "\n",
    "それぞれのパラメータによる損失関数の微分は以下のようになります。一般に以下の式で定式化されます。\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial L}{\\partial a} &= \\frac{1}{n} \\bigr( -\\sum_{i=1}^{n} y_i x_i + \\sum_{i=1}^{n} (a x_i + b)x_i \\bigr)  \\\\\n",
    "  \\frac{\\partial L}{\\partial b} &= \\frac{1}{n} \\bigr( -\\sum_{i=1}^{n} y_i     + \\sum_{i=1}^{n} (a x_i + b) \\bigr)\n",
    "\\end{align}\n",
    "\n",
    "これを最急降下法に適用すると更新式ができます。\n",
    "\n",
    "もちろんこのまま計算してもいいですが、切片と係数では微妙に形が違いますね。これを解決するために、切片も係数と考え、この係数に対しては常に値１が与えられると考えることで統一的に処理できます。\n",
    "\n",
    "最終的に得られる更新式は以下の通りです。\n",
    "\\begin{align}\n",
    "  a_{new} &= a_{old} - \\frac{\\alpha}{n} \\Bigr( - \\sum_{i=1}^{n} y_i x_i + \\sum_{i=1}^{n} (a_{old} x_i + b_{old})x_i \\bigl)  \\\\\n",
    "  &= a_{old} - \\frac{\\alpha}{n} \\Bigr( - \\sum_{i=1}^{n} y_i x_i + \\sum_{i=1}^{n} \\hat{y_i} x_i \\Bigl)  \\\\\n",
    "  &=  a_{old} - \\frac{\\alpha}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i) x_i  \\\\\n",
    "\\end{align}\n",
    "\n",
    "上に倣って変形すると、$b$の更新式は\n",
    "\n",
    "\\begin{align}\n",
    "  b_{new} &= b_{old} - \\frac{\\alpha}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i) \\times 1\n",
    "\\end{align}\n",
    "\n",
    "となります。\n",
    "\n",
    "上式を繰り返すことによって、$a$ と $b$ が徐々に適切な値になって行きます。この時繰り返し回数(イテレーション回数)と学習率によってはうまく収束しません。適切な大きさに設定する必要があります。\n",
    "\n",
    "この更新式を実装するにあたって、様々な手法があると思いますが、まずは自分が一番簡単だと思う方法で実装するのがいいと思います。\n",
    "\n",
    "パラメータは $w(2,1)$ 行列、与えられる特徴量は１が一列目にすべて入っている $x(n,2)$ 行列、ラベルは $y(n,1)$ という形で設定しておくと数式通りなので、分かりやすくなると思います。\n",
    "\n",
    "## 線形回帰における多変量解析\n",
    "\n",
    "機械学習はビッグデータの活用といわれるように、一変数だけで解析することはほぼありません。複数の変量、つまり説明変数を用いることが一般的です。これは線形代数を用いることで簡単に記述することができます。\n",
    "\n",
    "### 多変数への拡張\n",
    "\n",
    "線形代数の知識が必要になるので、無理に導出を追う必要はありません。難しければ実装上必要な部分だけ確認してください。\n",
    "\n",
    "これまで考えていたモデルは $\\hat{y} = ax + b$ で、たった一変数のみで表すことができました。これを多変数モデルに拡張します。一般的にこれを```重回帰```といいます。\n",
    "\n",
    "特徴量が $d$ 個、サンプル数が $n$ 個ある場合の仮定関数は行列を使うことで以下のように表せます。\n",
    "\n",
    "\\begin{align}\n",
    "  \\hat{y} = xw\n",
    "\\end{align}\n",
    "\n",
    "行列の中身は以下の通りです。書籍によっては行と列の方向が違うことがありますが、今回はこのように定義します。\n",
    "\n",
    "\\begin{align}\n",
    "  \\hat{y} = \\begin{bmatrix}\n",
    "    \\hat{y_1} \\\\\n",
    "    \\hat{y_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{y_n}\n",
    "  \\end{bmatrix},\n",
    "  w = \\begin{bmatrix}\n",
    "    w_0 \\\\\n",
    "    w_1 \\\\\n",
    "    w_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    w_d\n",
    "  \\end{bmatrix},\n",
    "  x = \\begin{bmatrix}\n",
    "    x_{01} && x_{11} && \\cdots && x_{d1} \\\\\n",
    "    x_{02} && x_{12} && \\cdots && x_{d2} \\\\\n",
    "    \\vdots && \\vdots && \\ddots && \\vdots \\\\\n",
    "    x_{0n} && x_{1n} && \\cdots && x_{dn}\n",
    "  \\end{bmatrix} \n",
    "\\end{align}\n",
    "\n",
    "ここで、行列$x$の一列目成分$x_{0i}$はすべて１であることに注意してください。切片を考量しない場合はこの列を削除することができます。\n",
    "\n",
    "\n",
    "理解のため、この行列表現から、サンプル $i$ 番目を取り出します。\n",
    "\n",
    "\\begin{align}\n",
    "  \\hat{y_i} = w_{0} x_{0i} + w_{1} x_{1i} + w_{2} x_{2i} + ... + w_{d} x_{di}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "単回帰と同じように、評価関数、損失関数を定義することが可能です。\n",
    "\n",
    "\\begin{align}\n",
    "  J &= \\frac{1}{n}||y-\\hat{y}||^2  \\\\\n",
    "    &= \\frac{1}{n} (y-\\hat{y})^T(y-\\hat{y})  \\\\\n",
    "    &= \\frac{1}{n} (y-xw)^T(y-xw)  \\\\\n",
    "    &= \\frac{1}{n} (y^T y - 2w^T x^T y - w^T x^T x w)\n",
    "\\end{align}\n",
    "\n",
    "一変数の時と同じで、これを微分して最急降下法を適用します。\n",
    "損失関数 $L = \\frac{J}{2}$ とします。\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial L}{\\partial w} &= \\frac{1}{2n} \\frac{\\partial}{\\partial w} (y^T y - 2w^T x^T y - w^T x^T x w)  \\\\\n",
    "  &= \\frac{1}{2n} (2x^T y - 2x^T x w)  \\\\\n",
    "  &= \\frac{1}{n}  x^T(y - x w)  \\\\\n",
    "  &= \\frac{1}{n}  x^T(y-\\hat{y})\n",
    "\\end{align}\n",
    "\n",
    "となるので、更新式は\n",
    "\n",
    "\\begin{align}\n",
    "  w_j = w_j - \\frac{\\alpha}{n} \\sum_{i=1}^{n} (y_i-\\hat{y_i})x_{ji}\n",
    "\\end{align}\n",
    "\n",
    "となります。\n",
    "\n",
    "あえて、パラメータを一つに絞って更新式を記述しましたが、行列のままでも計算できます。計算機では行列計算に落とし込むことで処理時間の短縮が可能なので、余裕があれば行列のまま実装することを推奨します。\n",
    "\n",
    "\n",
    "### 多項式への拡張\n",
    "\n",
    "線形回帰は多項式を使って近似することもできます。例えば鉛直投げ上げにおいて、初速度 $v_0$ が与えられた時に到達点の高さを予測したいとします。鉛直投げ上げにおける最高到達点は $y=\\frac{v_0^2}{2g}$ となることは知られているのですが、仮にこれを機械学習を用いて予測する場合、この式に近似させるためには多項式を使う必要があると言えます。\n",
    "\n",
    "多項式に対応することは簡単で、重回帰で使った行列を以下のようにすることで任意の項までの近似式を作ることができます。\n",
    "\n",
    "\\begin{align}\n",
    "  x = \\begin{bmatrix}\n",
    "    1 && x_{1} && x_{1}^2 && \\cdots && x_{1}^d \\\\\n",
    "    1 && x_{2} && x_{2}^2 && \\cdots && x_{2}^d \\\\\n",
    "    \\vdots && \\vdots && \\vdots && \\ddots && \\vdots \\\\\n",
    "    1 && x_{n} && x_{n}^2 && \\cdots && x_{n}^d\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "パラメータの最適化はこれまでと同じ方法で行えます。また、単純に２乗３乗と特徴量を作っていますが、$sin(x)$ でも $cos(x)$ でも $log(x)$ でも問題なく学習できますし、$\\frac{x+z}{2}$ のような別の特徴量と混ぜたものでも構わないです。これは言ってしまえば特徴量を自分で作る作業にほかなりません。\n",
    "\n",
    "つまり、線形回帰における「線形」とは目的変数がパラメータと対応する特徴量の積の線形結合で表せるということを言っています。もっと単純には $\\hat{y} = xw$ と記述できる。ということです。\n",
    "\n",
    "これらのモデルを総称して```(一般)線形モデル```ということもあります。\n",
    "\n",
    "## 一般化線形モデルに向けて\n",
    "\n",
    "一般化線形モデルは```Generalized Linear Model(GLM)```(以下GLM)と呼ばれます。実は\\一般線形モデルには仮定が含まれています。それは誤差が正規分布に従うものであり、リンク関数が恒等関数である。という仮定です。\n",
    "\n",
    "例えば、売り上げの予測や観客動員数、価格の予測など、こういったモデルはマイナスの値をとらないです。しかし、線形回帰では直線なので、入力によってはマイナスの値をとってしまいます。また、花の分類など、分類問題においては確率の解釈ができない値を返すことがあると予想できます。直感的にこれらのモデルの誤差が正規分布に従っているとは考えにくいです。\n",
    "\n",
    "こういったモデルに対応するようなものを考えるためのものがGLMです。\n",
    "次の章ではロジスティック回帰を例に解説していきます。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
