{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!---- deploy info ---->\n",
    "\n",
    "# ニューラルネットワークの基礎\n",
    "\n",
    "------------\n",
    "## このテキストについて\n",
    "\n",
    "### テキストの目的\n",
    "\n",
    "- ニューラルネットワークの基礎を知る\n",
    "- 順伝播法を理解する\n",
    "- 逆伝播法を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "\n",
    "基本である重回帰や一般化線形モデルを意識しながら完全に新しい手法として覚えるのではなく、今までの知識で記述できることを感じましょう。\n",
    "\n",
    "------------\n",
    "\n",
    "## ニューラルネットワークとは\n",
    "\n",
    "一般にNeural Network(以下NN)はGeneralized Linear Model(以下GLM)の文脈で学ばれます。GLMの多出力・多層モデルがNNと同値関係あるからです。\n",
    "\n",
    "一般化線形モデルを思い出してみましょう。\n",
    "\n",
    "### 一般化線形モデル\n",
    "\n",
    "行列を以下のような形で定義します。\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "  \\begin{bmatrix}\n",
    "    \\hat{y_1} \\\\\n",
    "    \\hat{y_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{y_n}\n",
    "  \\end{bmatrix},\n",
    "W = \n",
    "  \\begin{bmatrix}\n",
    "    w_0 \\\\\n",
    "    w_1 \\\\\n",
    "    w_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    w_d\n",
    "  \\end{bmatrix},\n",
    "X = \n",
    "  \\begin{bmatrix}\n",
    "    x_{01} & x_{11} & \\cdots & x_{d1} \\\\\n",
    "    x_{02} & x_{12} & \\cdots & x_{d2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{0n} & x_{1n} & \\cdots & x_{dn}\n",
    "  \\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "一般化線形モデルの枠組みでは、線形予測子 $\\eta = XW$ をさらに関数に入れることで、誤差の分布を解析対象に沿うようなものにするものでした。対数尤度などを駆使して微分することで、最適なパラメータを求めていました。\n",
    "\n",
    "### ニューラルネットワーク\n",
    "\n",
    "GLMの多出力・多層モデルがNNと同値関係あるといいました。\n",
    "これは、ニューラルネットワーク各層は全てGLMの多出力モデルと解釈できるためです。\n",
    "\n",
    "*補足*\n",
    ">線形写像というものがあります。簡単な例を出すと $f(x,y,z)=ax + by + cz$ のような関数は線形写像になります。この時、この関数 $f$ は 3次元ベクトル $(x,y,z)$ から、係数をかけて線形結合することでスカラー値にしています。\\\n",
    "関数がどのような写像をするのかわかりやすくするため $f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^1$ と書くことが多いです。\\\n",
    "見慣れないかもしれませんが仮に $f(x,y,z)=(ax + by + cz,dx + ey + fz)^T$となる写像があるならこれは $f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^2$ と書けます。\\\n",
    "この係数を取り出して、並べたものを行列として行列演算を定義しています。\\\n",
    "今回の例では、\n",
    "$$\n",
    "A =\n",
    "  \\begin{bmatrix}\n",
    "    a & b & c \\\\\n",
    "    d & e & f\n",
    "  \\end{bmatrix},\n",
    "X = \n",
    "  \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    z\n",
    "  \\end{bmatrix},\\\\\n",
    "AX =\n",
    "  \\begin{bmatrix}\n",
    "    ax + by + cz \\\\\n",
    "    dx + ey + fz\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "となります。\\\n",
    "一般で考える場合、任意の自然数 $m,n$ において実数値行列$(m,n)$ は線形写像 $f:\\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ を実現します。複素数行列の場合には $f:\\mathbb{C}^m \\rightarrow \\mathbb{C}^n$ となります。\n",
    "余裕があればそういった観点も踏まえて、行列を考えてみましょう。\n",
    "\n",
    "今回は行列を以下のように定義します。\n",
    "\n",
    "$$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "    X_{11} & X_{12} & \\cdots & w_{1d} \\\\\n",
    "    X_{21} & w_{22} & \\cdots & w_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    X_{n1} & X_{n2} & \\cdots & w_{nd}\n",
    "  \\end{bmatrix},\n",
    "W =\n",
    "  \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1h} \\\\\n",
    "    w_{21} & w_{22} & \\cdots & w_{2h} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{d1} & w_{d2} & \\cdots & w_{dh}\n",
    "  \\end{bmatrix},\n",
    "b =\n",
    "  \\begin{bmatrix}\n",
    "    b_{1} \\\\\n",
    "    b_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{h}\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{(n \\times d)},\n",
    "W \\in \\mathbb{R}^{(d \\times h)},\n",
    "b \\in \\mathbb{R}^{(h)},\n",
    "\\hat{y} \\in \\mathbb{R}^{(n \\times h)}\n",
    "$$\n",
    "\n",
    "$n$ : バッチサイズ\n",
    "\n",
    "$d$ : 入力される特徴量の数\n",
    "\n",
    "$h$ : 出力する特徴量の数\n",
    "\n",
    "$d$ 次元空間にある $n$ 個のデータを $h$ 次元空間に写像するというイメージです。\n",
    "線形回帰のイメージでは、求めたい目的変数が $h$ 個あるような状況で、それを同時に作るようなものです。\n",
    "\n",
    "バイアス項は次元があっていませんが、バッチ方向に同じ値を作って $b \\in \\mathbb{R}^{(n \\times h)}$ と考えてください。実装時はブロードキャストでこれを実現してください。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
