{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!---- deploy info ---->\n",
    "\n",
    "# ニューラルネットワークの基礎\n",
    "\n",
    "------------\n",
    "## このテキストについて\n",
    "\n",
    "### テキストの目的\n",
    "\n",
    "- ニューラルネットワークの基礎を知る\n",
    "- 順伝播法を理解する\n",
    "- 逆伝播法を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "\n",
    "基本である重回帰や一般化線形モデルを意識しながら完全に新しい手法として覚えるのではなく、今までの知識で記述できることを感じましょう。\n",
    "\n",
    "------------\n",
    "\n",
    "## ニューラルネットワークとは\n",
    "\n",
    "一般にNeural NetworkはGeneralized Linear Modelの文脈で学ばれます。GLMの多出力・多層モデルがNNと同値関係あるからです。\n",
    "\n",
    "一般化線形モデルを思い出してみましょう。\n",
    "\n",
    "### 一般化線形モデル\n",
    "\n",
    "行列を以下のような形で定義します。\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "  \\begin{bmatrix}\n",
    "    \\hat{y_1} \\\\\n",
    "    \\hat{y_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{y_n}\n",
    "  \\end{bmatrix},\n",
    "W = \n",
    "  \\begin{bmatrix}\n",
    "    w_0 \\\\\n",
    "    w_1 \\\\\n",
    "    w_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    w_d\n",
    "  \\end{bmatrix},\n",
    "X = \n",
    "  \\begin{bmatrix}\n",
    "    x_{01} & x_{11} & \\cdots & x_{d1} \\\\\n",
    "    x_{02} & x_{12} & \\cdots & x_{d2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{0n} & x_{1n} & \\cdots & x_{dn}\n",
    "  \\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "一般化線形モデルの枠組みでは、線形予測子 $\\eta = XW$ をさらに関数に入れることで、誤差の分布を解析対象に沿うようなものにするものでした。\n",
    "\n",
    "### 線形代数\n",
    "\n",
    "GLMの多出力・多層モデルがNNと同値関係あるといいました。\n",
    "これは、ニューラルネットワーク各層は全てGLMの多出力モデルと解釈できるためです。\n",
    "\n",
    "行列を以下のように定義ます。\n",
    "\n",
    "$$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "    X_{11} & X_{12} & \\cdots & w_{1d} \\\\\n",
    "    X_{21} & w_{22} & \\cdots & w_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    X_{n1} & X_{n2} & \\cdots & w_{nd}\n",
    "  \\end{bmatrix},\n",
    "W =\n",
    "  \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\cdots & w_{1h} \\\\\n",
    "    w_{21} & w_{22} & \\cdots & w_{2h} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{d1} & w_{d2} & \\cdots & w_{dh}\n",
    "  \\end{bmatrix},\n",
    "B =\n",
    "  \\begin{bmatrix}\n",
    "    B_{1} \\\\\n",
    "    B_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    B_{h}\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + B\n",
    "$$\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{(n \\times d)},\n",
    "W \\in \\mathbb{R}^{(d \\times h)},\n",
    "b \\in \\mathbb{R}^{(h)},\n",
    "\\hat{y} \\in \\mathbb{R}^{(n \\times h)}\n",
    "$$\n",
    "\n",
    "$n$ : バッチサイズ\n",
    "\n",
    "$d$ : 入力される特徴量の数\n",
    "\n",
    "$h$ : 出力する特徴量の数\n",
    "\n",
    "$d$ 次元空間にある $n$ 個のデータを $h$ 次元空間に写像するというイメージです。\n",
    "線形回帰のイメージでは、求めたい目的変数が $h$ 個あるような状況で、それを同時に作るようなものです。\n",
    "\n",
    "バイアス項は次元が合っていませんが、バッチ方向に同じ値を作って $B \\in \\mathbb{R}^{(n \\times h)}$ と考えてください。\n",
    "\n",
    "実装時はブロードキャストでこれを実現することができます。\n",
    "\n",
    "*補足*\n",
    ">線形写像というものがあります。簡単な例を出すと $f(x,y,z)=ax + by + cz$ のような関数は線形写像になります。この時、この関数 $f$ は ３次元ベクトル $(x,y,z)$ から、係数をかけて線形結合することでスカラー値にしています。\\\n",
    "こうした写像がどのような写像をするのかわかりやすく記述するため $f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^1$ と書くことが多いです。\\\n",
    "見慣れないかもしれませんが、仮に $f(x,y,z)=(ax + by + cz,dx + ey + fz)$となる写像があるならこれは $f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^2$ と書けます。\\\n",
    "この係数を取り出して、並べたものを行列として行列演算を定義しています。\\\n",
    "今回の例では、\n",
    "$$\n",
    "A =\n",
    "  \\begin{bmatrix}\n",
    "    a & b & c \\\\\n",
    "    d & e & f\n",
    "  \\end{bmatrix},\n",
    "X = \n",
    "  \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    z\n",
    "  \\end{bmatrix},\\\\\n",
    "AX =\n",
    "  \\begin{bmatrix}\n",
    "    ax + by + cz \\\\\n",
    "    dx + ey + fz\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "となります。\\\n",
    "一般で考える場合、任意の自然数 $m,n$ において実数値行列$(m,n)$ は線形写像 $f:\\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ を実現します。複素数行列の場合には $f:\\mathbb{C}^m \\rightarrow \\mathbb{C}^n$ となります。\n",
    "余裕があればそういった観点も踏まえて、行列を考えてみましょう。\n",
    "\n",
    "## ニューラルネットワーク\n",
    "\n",
    "今回は活性化関数には$sigmoid$関数で、隠れ層が１層のNNを考えます。\n",
    "\n",
    "まずはモデルが小さくても済むようにIrisを分類するモデルを考えることにしましょう。設定としては、まずは簡単にバッチサイズ１で中間層は５つのノードを持っているNNを考えることにします。出力層には多値分類のためソフトマックス関数を使います。\n",
    "\n",
    "ソフトマックス関数とは $n$ 次元ベクトル $x$ のk番目の値を\n",
    "$$\n",
    "y_k = softmax(x_k) = \\frac{exp(x_k)}{\\sum^{n}_{i=1} exp(x_i)}\n",
    "$$\n",
    "このように正規化して確率解釈するための関数です。\n",
    "イメージとしてはシグモイド関数の多値バージョンです。\n",
    "\n",
    "Irisは特徴量が４で、ラベルは３種類なので、入力層のノード数と出力層のノード数はそれぞれ４、３になります。\n",
    "\n",
    "このような図になるNNですね。\n",
    "<a href=\"https://diveintocode.gyazo.com/d9ccc2a1ffe4aecdcf3dd469a1ca9c70\"><img src=\"https://t.gyazo.com/teams/diveintocode/8f177f66ac7dbdc5dd33babd80b60a84.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "\n",
    "\n",
    "この時それぞれの行列の形は\n",
    "\n",
    "入力$X(1,4)$\n",
    "\n",
    "$W_1(4,5),B_1(1,5)$\n",
    "\n",
    "$W_2(5,3),B_2(1,3)$\n",
    "\n",
    "となります。\n",
    "仮にバッチサイズが１ではなくても、計算が定義できることを確認してください。\n",
    "\n",
    "## 順伝播\n",
    "\n",
    "順伝播は簡単です。\n",
    "\n",
    "入力層 $\\rightarrow$ 中間層\n",
    "$$\n",
    "A_1 = XW_1 + B_1\\\\\n",
    "Z_1 = f_1(A_1) = A_1W_1 + B_1\n",
    "$$\n",
    "\n",
    "中間層 $\\rightarrow$ 出力層\n",
    "$$\n",
    "A_2 = Z_1W_2 + B_2\\\\\n",
    "y = f_2(A_2) = softmax(A_2)\n",
    "$$\n",
    "\n",
    "このように入力からの値を前に伝えていきます。\n",
    "\n",
    "## 逆伝播\n",
    "\n",
    "逆伝播では出力層から考えていきます。\n",
    "出力層の先にはラベルがあります。\n",
    "今回はIrisデータセットを例にしているので、\n",
    "$$\n",
    "t = \\begin{bmatrix}1 & 0 & 0\\end{bmatrix}\n",
    "$$\n",
    "等がラベルになります。\n",
    "\n",
    "交差エントロピー誤差 $E$ は以下のように定義されます。\n",
    "\n",
    "$$\n",
    "E = - \\sum_{b=1}^{1} \\sum_{k=1}^3 t_k^{(b)} \\log{y_k^{(b)}}\n",
    "$$\n",
    "\n",
    "外側の $i$ についてのシグマはバッチに対応しています。今回はバッチサイズを１として考えているので、必要ありませんが、一般化のため書いています。\n",
    "\n",
    "これまでと同じようにこれを最小にするようにパラメータを最適させていきます。\n",
    "\n",
    "最適化させたいパラメータは $W_1, B_1, W_2, B_2$ の四つです。\n",
    "\n",
    "後ろの層から求めていきましょう。\n",
    "\n",
    "偏微分における連鎖律という概念を利用して以下のように式変形できます。行列の微分は定義によって様々なのですが、今回は以下のようにして定義することができます。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W_2} = \\frac{\\partial A_2}{\\partial W_2}^T \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial A_2}\n",
    "$$\n",
    "\n",
    "ベクトルの微分になるので難しいかもしれませんが、納得感を得るだけでも大丈夫です。まずは右辺の$\\frac{\\partial E}{\\partial y}$ を計算すると、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial y_c} = - \\frac{\\partial}{\\partial y_c} \\sum_{k=1}^3 t_k \\log{y_k}\\\\\n",
    "= - \\frac{t_c}{y_c}\n",
    "$$\n",
    "\n",
    "となります。$t_c=\\{1,0\\},y_c=(0,1)$です。\n",
    "\n",
    "*数学的記法について補足*\n",
    ">x=(0,1)：xが０より大きく１より小さい値をとる\\\n",
    "x={0,1}：xが０か１のどちらかをとる\n",
    "\n",
    "次に $\\frac{\\partial y}{\\partial A_2}$ を $i=c$ であるときと、そうでないときで計算が異なることに注意して\n",
    "\n",
    "$i=c$の時、\n",
    "($A_2$ の $i$ の要素を $A_{2i}$ とします。)\n",
    "$$\n",
    "\\frac{\\partial y_c}{\\partial A_{2c}} = \\frac{\\partial}{\\partial A_{2c}} \\frac{exp(A_{2c})}{\\sum^{3}_{i=1} exp(A_{2i})}\\\\\n",
    "= \\frac{exp(A_{2c}) \\sum^{3}_{i=1} exp(A_{2i}) - exp(A_{2c})^2}{\\bigl(\\sum^{3}_{i=1} exp(A_{2i})\\bigr)^2}\\\\\n",
    "= \\frac{exp(A_{2c}) \\bigl( \\sum^{3}_{i=1} exp(A_{2i}) - exp(A_{2c}) \\bigr)}{\\bigl(\\sum^{3}_{i=1} exp(A_{2i})\\bigr)^2}\\\\\n",
    "= y_c (1-y_c)\n",
    "$$\n",
    "\n",
    "$i=c$ではない時、\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial A_{2c}} = \\frac{\\partial}{\\partial A_{2c}} \\frac{exp(A_{2i})}{\\sum^{3}_{i=1} exp(A_{2i})}\\\\\n",
    "= \\frac{- exp(A_{2i})exp(A_{2c})}{\\bigl(\\sum^{3}_{i=1} exp(A_{2i})\\bigr)^2}\\\\\n",
    "= - y_c y_i\n",
    "$$\n",
    "\n",
    "これらをすべて行列表現にすると以下のようになることが示されました。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial A_2} \n",
    "= - \\begin{bmatrix}\\frac{t_0}{y_0} & \\frac{t_1}{y_1} & \\frac{t_2}{y_2}\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  y_0 (1-y_0) & - y_0 y_1   & -y_0 y_2\\\\\n",
    "  - y_1 y_0   & y_1 (1-y_1) & -y_1 y_2\\\\\n",
    "  - y_2 y_0   & - y_2 y_1   & y_2 (1-y_2)\n",
    "\\end{bmatrix}\\\\\n",
    "=\\begin{bmatrix}\n",
    "- t_0 (1-y_0) + t_1 y_0     + t_2 y_0\\\\\n",
    "  t_0 y_1       - t_1 (1-y_1) + t_2 y_1\\\\\n",
    "  t_0 y_2       + t_1 y_2     - y_2 (1-y_2)\n",
    "\\end{bmatrix}^T\\\\\n",
    "= \\begin{bmatrix}\n",
    "  y_0 - t_0\\\\\n",
    "  y_1 - t_1\\\\\n",
    "  y_2 - t_2\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "このように簡単な形になります。\n",
    "\n",
    "最後に $\\frac{\\partial A_2}{\\partial W_2}$ を求めます。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial A_2}{\\partial W_2} = \\frac{\\partial}{\\partial W_2} (Z_1W_2 + B_2)\n",
    "= Z_1\n",
    "$$\n",
    "\n",
    "したがって、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W_2} = \\frac{\\partial A_2}{\\partial W_2}^T \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial A_2}\n",
    "= Z_1^T(y-t)\n",
    "$$\n",
    "\n",
    "それぞれの行列の形を確認しましょう。\n",
    "$Z_1 \\in \\mathbb{R}^{(1 \\times 5)},(y-t) \\in \\mathbb{R}^{(1 \\times 3)}$ なので、$W_2 = Z_1(y-t) \\in \\mathbb{R}^{(5 \\times 3)}$ となり問題なく一致しています。\n",
    "\n",
    "$B_2$ 更新も考えましょう。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial A_2}{\\partial B_2} = \\frac{\\partial}{\\partial B_2} (Z_1W_2 + B_2)\n",
    "= I\n",
    "$$\n",
    "\n",
    "単位行列が出てきます。したがって以下のようにまとめられます。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial B_2} = \\frac{\\partial A_2}{\\partial B_2}^T \\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial A_2}\n",
    "= (y-t)\n",
    "$$\n",
    "\n",
    "\n",
    "仮にバッチ数が増えても簡単に拡張できます。行方向にデータを増やせばいいだけです。\n",
    "\n",
    "最終的に得られる $W_2,B_2$ の更新式は\n",
    "$$\n",
    "W_2^{(new)}= W_2^{(old)} - Z_1(y-t)\\\\\n",
    "B_2^{(new)}= B_2^{(old)} - (y-t)\n",
    "$$\n",
    "\n",
    "です。\n",
    "\n",
    "次に$W_1,B_1$の更新を考えてみましょう。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W_1} = \\frac{\\partial A_1}{\\partial W_1}^T \\frac{\\partial E}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial A_1}\n",
    "$$\n",
    "\n",
    "ニューラルネットワークにおけるもっとも重要な概念が$\\frac{\\partial E}{\\partial A_i}=\\frac{\\partial E}{\\partial Z_i} \\frac{\\partial Z_i}{\\partial A_i}$になります。ここさえ押さえられればDeepも問題なく実装できます。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial Z_1} = \\frac{\\partial E}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_1}\\\\\n",
    "= (y-t)W_2^T\n",
    "$$\n",
    "\n",
    "合わせて$\\frac{\\partial Z_1}{\\partial A_1}$も計算しておきます。これは活性化関数を微分するだけなので、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Z_1}{\\partial A_1} = (1-sigmoid(A_1))sigmoid(A_1)\n",
    "$$\n",
    "\n",
    "となります。最後に$\\frac{\\partial A_1}{\\partial W_1}^T$は、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial A_1}{\\partial W_1}^T = X^T\n",
    "$$\n",
    "\n",
    "となり、一層目での更新式が得られるようになりました。バイアスも同様に\n",
    "\n",
    "$$\n",
    "\\frac{\\partial A_1}{\\partial B_1}^T = I\n",
    "$$\n",
    "\n",
    "となるので、簡単に求まります。\n",
    "\n",
    "Deepに発展する際、$\\frac{\\partial E}{\\partial A_i}$を常に上に渡して誤差を伝えていくことになります。\n",
    "仮に上にもう一層あると考えてみましょう。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W_0} = \\frac{\\partial A_0}{\\partial W_0}^T \\frac{\\partial E}{\\partial Z_0} \\frac{\\partial Z_0}{\\partial A_0}\n",
    "$$\n",
    "\n",
    "このうち$\\frac{\\partial E}{\\partial Z_0}$ここを考えてみます。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial Z_0} = \\frac{\\partial E}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_0}\\\\\n",
    "= \\frac{\\partial E}{\\partial A_1} W_1^T\n",
    "$$\n",
    "\n",
    "と、やはりここで必要になるわけです。$\\frac{\\partial E}{\\partial A_1}$を実際に求めてみると、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial A_1} = \\frac{\\partial E}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial A_1}\\\\\n",
    "= (y-t)W_2^T \\odot (1-sigmoid(A_1))sigmoid(A_1)\n",
    "$$\n",
    "\n",
    "と求めることができるわけです。\n",
    "\n",
    "オブジェクト指向を意識してLayerをクラスにして実装する場合の注意としては、その層でしか計算できないものがあるので、それをしっかりと別々にインスタンス変数などで保持して、上の層で必要なときに上の層で引き出せるようにしましょう。\n",
    "\n",
    "## ニューラルネットワークを利用した次元圧縮\n",
    "\n",
    "Auto Encoder(AE):出力層に入力に使った値を使った次元圧縮器\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/d9ccc2a1ffe4aecdcf3dd469a1ca9c70\"><img src=\"https://t.gyazo.com/teams/diveintocode/aaf063453957877d902a9885dae7c35a.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "入力特徴量を任意の次元に小さくした後に、入力特徴量をできるだけ再現できるように学習させることで、最も情報が落ちないように情報を圧縮する手法です。\n",
    "\n",
    "あまりAEを動かすことはないかと思いますが、VAEやそのほかの潜在空間を得るタイプのアーキテクチャを持つモデルはたくさんあります。その基本となっている考え方です。(厳密にいうとVAEはAEの文脈から生まれたものではありませんが）\n",
    "\n",
    "活性化関数をすべて恒等関数にすると、PCAと同値になることが示されています。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
